					‚úÖüíïPROJECT END TO END ML STUDENT MARK PREDICTOR


‚ù§Ô∏è14/10/2024
	Step 1:
		Created a repo in GitHub
		Created a folder in local 
		Opened the folder in vs code and created a venv and activated
		In terminal - CREATING A EMPTY GIT repo USING - (git init) command   ( As per the GitHub instructions)
		Create a readme.md file in the folder -- add some content  adding this readme.md file in git repo by ( git add README.MD)
		commiting the changes - git commit -m "my first commit"
		Adding the GitHub commands provided by git 
		AT last to push to files to the repo adding the push command 
		Before this we need to give command git configurations
		i.e user name and user email (if we are pushing it for the 1st time)
		Next create .gitignore file in GitHub select language AS PYTHON
	Step 2:
	       Creating setup.py and requirements.txt files in local
	ABOUT SETUP.PY
		THIS FILE IS USED TO CONVERT OUT PORJECT TO A PACKAGE , We can use this package later on where ever needed.
	
step3:
	Create a src folder -> create a __init__.py 
	if we want to found this src folder as a package we should have a file called __init__.py
	whenever this setup.py find_packages will run it will go and see in how many folders there is __init.py__ files are present so directly it considers this src folder as a package and try to build __init__.py. Once the build is over we can use this as a package 
	Our entire project code will be on this folder (src)

step4:
	create Def get_requirements()in setup.py
	This function is to automate and collect the packages used in requirements.txt 
	NOTE:
		WE WILL WRITE "-e ." in REQ.TXT TO TRIGGER THE SETUP.PY FILE 
	THIS func return the list of requirements and pass it to the install_requires parameter
	this func will extract the packages by getting the file path as arg and use readlines() to read each line while using this function it will create a "\n" str in each line so we need to remove this and also handle the '-e ." so that this shouldn't include in the returing list 
	
step 5:
	Once we run pip install -r requirements.txt
	It will automatically triggers setup.py and installs and it will create a package name "mlproject.egg-info"
	THEN WE COMMIT THE CHANGES TO THE GIT BY 	git add .
	Commiting the changes - git commit -m "setup nd req"
	Pushing the files to the repo -- git push -u origin main

	STEP6:  PROJECT STRUCTURE
	NOTE:
		ANY FOLDER WE CREATE THERE SHOULD BE __INIT__.PY FILE
		CREATING PROJECT STRUCTURE
		CREATE COMPONENTS FOLDER
			ADD __INIT__PY
			ADD DATA_INGESTION.PY
			ADD DATA_TRANSFORMATION.PY
			ADD MODEL_TRAIN.PY
		CREATE  PIPELINE FOLDER
			ADD PREDICT_PIPELINE.PY
			ADD TRAIN_PIPELINE.PY
		THESE PIPELINE FILES TRIGGER THE COMPONENTS 

		AT LAST CREATE COMMON FILES SUCH AS 
			LOGGER.PY
			UTILS.PY ( HERE WE HAVE THE COMMON FUNCTIONS)
			EXCEPTION.PY
	STEP 7:
		WRITE CODE FOR LOGG AND EXCEPTION
	
‚ù§Ô∏è15/10/2024-
	Worked on the  data preprocessing EDA 
‚ù§Ô∏è16/10/2024
	We are using regression models because the scores are in regression


	Working on model trainer.ipynb 
	Here we are assumed to predict the math score of the student 
	SO x (Independent features) are except math_Score column
	Y (dependent features) are math_score column
	NOTE:
		we have to encode the data 
		We use ONEHOTENCODING  for the less number of features dataset 
		We use TARGETEDENCODING for the dataset which has large number of features 
		We use STANDARDSCALAR for the numerical features
		ONEHOTENCODING for the categorical features
	Creating a pre processor pipeline for encoding in which first ONEHOTENCODING takes place followed by STANDARDSCALAR
		We use COLUMNTRANSFORMERS these are responsible for transforming your column datas


	DATAINGESTION.PY
		‚ùå We have been using DATACLASS decorator imp interview question why it has used.
	DATATRASNFORMATION.PY
		‚ùåwhat is the diff between fit_transform and transform (interview question)
			‚úÖfit is used for learning the parameters from the data, but it does not alter the data.
			‚úÖtransform only applies the learned parameters to the data, and it can only be used after fit.
			‚úÖfit_transform is convenient when you want to both learn from and transform the data in a single step.
		‚ùåwhat is the use of NP.C_   (interview question)

			‚úÖIn NumPy, np.c_ is a shorthand for column-wise concatenation, and it‚Äôs often used to stack 1-D arrays as 					columns into a 2-D array.
			‚úÖKey Features of np.c_
				Column-wise Stacking: It takes multiple arrays and stacks them as columns in a 2-D array.
				Array Creation: It‚Äôs useful for quickly creating 2-D arrays, especially when you need to combine multiple 				arrays.
				Handling 1-D Arrays: If you pass 1-D arrays, np.c_ automatically converts them to 2-D, where each array 				becomes a column

17/10/2024
	MODEL TRAINER.PY
		Importing all the required modules

18/10/2024
	Studying about the dockers series  







		